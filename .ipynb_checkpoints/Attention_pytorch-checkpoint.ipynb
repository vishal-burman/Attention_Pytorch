{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de_core_news_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 133 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: de-core-news-sm\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.1.0-py3-none-any.whl size=11073066 sha256=9bdae4b917cab9f06d7c743b175ac0f950035d46056c7630f7c22246b1f8c12d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yapx0c5f/wheels/19/c6/ac/c9e47e5851255b175d864b74fef866e331c8375a5517a912cb\n",
      "Successfully built de-core-news-sm\n",
      "Installing collected packages: de-core-news-sm\n",
      "  Attempting uninstall: de-core-news-sm\n",
      "    Found existing installation: de-core-news-sm 2.2.5\n",
      "    Uninstalling de-core-news-sm-2.2.5:\n",
      "      Successfully uninstalled de-core-news-sm-2.2.5\n",
      "Successfully installed de-core-news-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/vishal/Desktop/pytorch_gpu/pytorch_gpu/lib/python3.7/site-packages/de_core_news_sm\n",
      "-->\n",
      "/home/vishal/Desktop/pytorch_gpu/pytorch_gpu/lib/python3.7/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "# ! python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 125 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-py3-none-any.whl size=11074433 sha256=e03ca909e6758da794722a633f59ba13f600af9add4c22577d6f30cad73916e6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ve4z2h3u/wheels/59/4f/8c/0dbaab09a776d1fa3740e9465078bfd903cc22f3985382b496\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.5\n",
      "    Uninstalling en-core-web-sm-2.2.5:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.5\n",
      "Successfully installed en-core-web-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/vishal/Desktop/pytorch_gpu/pytorch_gpu/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/home/vishal/Desktop/pytorch_gpu/pytorch_gpu/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "# ! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizers\n",
    "spacy_de=spacy.load('de')\n",
    "spacy_en=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\" Tokenizes german text from a string into a list of strings \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\" Tokenizes english text from a string into a list of strings \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model expects data to be fed in with batch dimension first, so we use batch_first=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC=Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
    "\n",
    "TRG=Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<sos>', lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the Multi30k dataset and build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data=Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we define the device and the data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "\n",
    "# train_iterator, valid_iterator, test_iterator=BucketIterator.splits((train_data, valid_data, test_data), batch_sizes=BATCH_SIZE, device=device)\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=100):\n",
    "        super.__init()\n",
    "        \n",
    "        self.device=device\n",
    "        self.tok_embedding=nn.Embedding(input_dim, output_dim)\n",
    "        self.pos_embedding=nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers=nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale=torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        # src= [batch_size, src_len]\n",
    "        # src_mask=[batch_size, src_len]\n",
    "        \n",
    "        batch_size=src.shape[0]\n",
    "        src_len=src.shape[1]\n",
    "        \n",
    "        pos=torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        # pos=[batch_size, src_len]\n",
    "        \n",
    "        src=self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        # src=[batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src=layer(src, src_mask)\n",
    "        \n",
    "        # src=[batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.layer_norm=nn.LayerNorm(hid_dim)\n",
    "        self.self_attention=MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward=PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        def forward(self, src, src_mask):\n",
    "            # src = [batch_size, src_len, hid_dim]\n",
    "            # src_mask = [batch_size, src_len]\n",
    "            \n",
    "            # self attention\n",
    "            _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "            \n",
    "            #dropout, residual connection and layer norm\n",
    "            src=self.layer_norm(src+self.dropout(_src))\n",
    "            # src = [batch_size, src_len, hid_dim]\n",
    "            \n",
    "            # positionwise feedforward\n",
    "            _src = self.positionwise_feedforward(src)\n",
    "            \n",
    "            # dropout, residual connection and layer norm\n",
    "            src=self.layer_norm(src + self.dropout(_src))\n",
    "            # src = [batch_size, src_len, hid_dim]\n",
    "            \n",
    "            return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim%n_heads=0\n",
    "        \n",
    "        self.hid_dim=hid_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.head_dim=hid_dim//n_heads\n",
    "        \n",
    "        self.fc_q=nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k=nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v=nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o=nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale=torch.sqrt(torch.FloatTensor([self.hid_dim])).to(device)\n",
    "        \n",
    "        def forward(self, query, key, value, mask=None):\n",
    "            \n",
    "            batch_size=query.shape[0]\n",
    "            \n",
    "            # query = [batch_size, query_len, hid_dim]\n",
    "            # key = [batch_size, key_len, hid_dim]\n",
    "            # value = [batch_size, value_len, hid_dim]\n",
    "            \n",
    "            Q = self.fc_q(query)\n",
    "            K = self.fc_k(key)\n",
    "            V = self.fc_v(value)\n",
    "            \n",
    "            Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "            K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "            V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "            # Q = [batch_size, query_len, hid_dim]\n",
    "            # K = [batch_size, key_len, hid_dim]\n",
    "            # V = [batch_size, value_len, hid_dim]\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
